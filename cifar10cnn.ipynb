{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 Image Classification using Convolutional Neural Networks\n",
    "We are going to be using pytorch, a deep learning framework, and utilize certain modules:\n",
    "- <span style=\"color: orange;\">**torch.nn**</span>: Module for building neural networks\n",
    "- <span style=\"color: orange;\">**torch.optim**</span>: Module for optimization algorithms (e.g. Adam, SGD)\n",
    "- <span style=\"color: orange;\">**torchvision**</span>: Module that provides our dataset and data transformation utilities.\n",
    "- <span style=\"color: orange;\">**DataLoader**</span> from torch.utils.data: Module for loading the dataset\n",
    "\n",
    "Other useful libraries:\n",
    "- <span style=\"color: orange;\">**matplotlib**</span>: Plotting & Calculations\n",
    "- <span style=\"color: orange;\">**numpy**</span>: Plotting & Calculations\n",
    "- <span style=\"color: orange;\">**time**</span>: To measure the training duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "#import matplotlib\n",
    "#matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# The variable \"device\" specifies the computational device \n",
    "# This is where we run our neural network on (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Transformations imposed on the training data improve generalization, the steps go as follows:\n",
    "- Cropping \n",
    "- Flipping\n",
    "- Conversion to tensor \n",
    "- Normalization\n",
    "\n",
    "However, when testing we only normalize the data:\n",
    "- Conversion to tensor \n",
    "- Normalization\n",
    "\n",
    "### Conversion to tensor\n",
    "\n",
    "**Converts the input image** from a PIL (Python Imaging Library) image or NumPy array into a PyTorch tensor\n",
    "- <span style=\"color: orange;\">**Rescales**</span> pixel values from the range [0, 255] (for images) to [0, 1] by dividing by 255.\n",
    "- <span style=\"color: orange;\">**Rearranges the image dimensions**</span> from (Height × Width × Channels) to (Channels × Height × Width) to match PyTorch's convention.\n",
    "\n",
    "The Tensor format is required for performing computations in PyTorch models.\n",
    "\n",
    "### Normalization\n",
    "For each channel (R, G, B), it subtracts the corresponding mean (0.4914, 0.4822, 0.4465) and divides by the corresponding standard deviation (0.2023, 0.1994, 0.2010):\n",
    "$$Normalized Value = \\frac{Pixel Value - Mean}{Standard Deviation}$$\n",
    "Normalization helps standardize the input data, which improves the convergence of the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "# Tranformations for the training dataset\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4), # Randomly crops image with padding\n",
    "    transforms.RandomHorizontalFlip(), # Randomly flips image horizontally\n",
    "    transforms.ToTensor(), # Converts image to tensor\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "# Tranformations for the test dataset\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(), # Converts image to tensor\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Loading dataset and applying previous transformations..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 training dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./cifar-10-batches-py', train=True,\n",
    "                                      download=True, transform=transform_train)\n",
    "trainloader = DataLoader(trainset, batch_size=128,\n",
    "                        shuffle=True, num_workers=2)\n",
    "\n",
    "# Load CIFAR-10 test dataset\n",
    "testset = torchvision.datasets.CIFAR10(root='./cifar-10-batches-py', train=False,\n",
    "                                     download=True, transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=100,\n",
    "                       shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset familiarization\n",
    "Fetch example images from the training dataset, along with the class they belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some example images:\n",
      "Labels:\n",
      "  frog  plane   deer    car\n",
      "  bird  horse  truck   deer\n"
     ]
    }
   ],
   "source": [
    "# Specify the classes\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Visualization function\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # reverse normalization\n",
    "    npimg = img.numpy() # convert to numpy array\n",
    "    npimg = np.clip(npimg, 0, 1)  # clip values to the valid range [0, 1]\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0))) # transpose to (H, W, C) for correct display\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Fetch some random images from the training dataset\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "print('Some example images:')\n",
    "imshow(torchvision.utils.make_grid(images[:8], nrow=4)) # select first 8 images, 4 images per row\n",
    "nrow = 4\n",
    "n_images = 8\n",
    "print(\"Labels:\")\n",
    "for i in range(0, n_images, nrow):\n",
    "    print(\" \".join(f\"{classes[labels[j]]:>6}\" for j in range(i, min(i + nrow, n_images))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks\n",
    "The CNN class defines the model architecture. There are 3 steps we have to go through:\n",
    "- Convolutional layers\n",
    "- Fully connected layers\n",
    "- Forward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN architecture\n",
    "class CNN(nn.Module):\n",
    "    # Initialize the architecture\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # First convolutional layer\n",
    "            nn.Conv2d(3, 32, 3, padding=1), # 3 input channels (R G B), 32 output channels, 3x3 kernel, 1 padding\n",
    "            nn.BatchNorm2d(32), # normalize the output of the previous layer to have a mean = 0 and standard deviation = 1\n",
    "            nn.ReLU(), # ReLU activation function: f(x) = max(0, x)\n",
    "\n",
    "            # Second convolutional layer\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2, 2),  # for each 2x2 window, take the maximum value\n",
    "            nn.Dropout(0.25), # set 25% of the neurons to zero\n",
    "\n",
    "            # Third convolutional layer\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Fourth convolutional layer\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            # Fifth convolutional layer\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Sixth convolutional layer\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(256 * 4 * 4, 512), # First fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 10) # Final fully connected layer, 512 input features, 10 output features (classes)\n",
    "        )\n",
    "\n",
    "    # Define the forward pass\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x) # Pass input through convolutional layers\n",
    "        x = x.view(x.size(0), -1) # Flatten the output of the convolutional layers\n",
    "        x = self.fc_layers(x) # Pass through fully connected layers\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "Calculates how well the model is doing by calculating the difference between the model's predictions and the actual target values(ground truth). The lower the loss the better the predictions. For classification problems we use:\n",
    "- Cross-Entropy Loss\n",
    "    - Measures difference between predicted probability distributions and actual classes\n",
    "    - Common for categorizing into discrete classes\n",
    "### Optimizer\n",
    "Adam (Adaptive Moment Estimation) is an optimization algorithm which updates the model's weights to minimize the loss function.\n",
    "### Learning Rate Scheduler\n",
    "Dynamically adjusts the learning rate during training:\n",
    "- Reduces learning rate when the performance peaks.\n",
    "- patience=3: Waits 3 epochs without improvement before triggering\n",
    "- factor=0.5: Reduces learning rate by half when triggered\n",
    "- min: monitors a metric and reduces learning rate when it stops decreasing\n",
    ">scheduler.step(test_loss) # When the scheduler gets called later in the code's main loop it is set as to minimize the test loss\n",
    "\n",
    "Example:\n",
    "1. Starting LR = 0.001\n",
    "2. After plateau (no improvement for 3 epochs):\n",
    "3. New LR = 0.001 * 0.5 = 0.0005\n",
    "4. After another plateau:\n",
    "5. New LR = 0.0005 * 0.5 = 0.00025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = CNN().to(device) # send model to device for training\n",
    "criterion = nn.CrossEntropyLoss() # Loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # Optimizer\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5) # Learning rate scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function\n",
    "The training process entails 4 basic steps:\n",
    "- <span style=\"color: orange;\">**Forward pass**</span>: We pass the images through the CNN neural network.\n",
    "- <span style=\"color: orange;\">**Loss Calculation**</span>: We calculate how far the model's predictions are from the ground truth.\n",
    "- <span style=\"color: orange;\">**Backward pass**</span>: We use back-propagation to compute the gradient of the loss function with respect to each weight in the network. This represents how much each weight contributed to the error.\n",
    "- <span style=\"color: orange;\">**Optimization**</span>: Using the Adam optimizer we update the weights in order to minimize loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, trainloader, criterion, optimizer, device):\n",
    "    model.train() # Set model to training mode\n",
    "    running_loss = 0.0 #\n",
    "    correct = 0 # Number of correctly predicted images\n",
    "    total = 0 # Total number of images\n",
    "    \n",
    "    # Iterate over the training dataset\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device) #data[0] is the image, data[1] is the label\n",
    "        \n",
    "        optimizer.zero_grad() # Reset the gradients of model parameters\n",
    "        outputs = model(inputs) # Forward pass\n",
    "        loss = criterion(outputs, labels) # Calculate loss\n",
    "        loss.backward() # Backward pass\n",
    "        optimizer.step() # Update weights\n",
    "        \n",
    "        # Track training statistics\n",
    "        running_loss += loss.item() # Add loss to running loss\n",
    "        _, predicted = outputs.max(1) # Get the class index with the highest probability\n",
    "        total += labels.size(0) # Add batch size to total\n",
    "        correct += predicted.eq(labels).sum().item() # Add number of correct predictions to correct\n",
    "        \n",
    "        # Print statistics every 100 mini-batches\n",
    "        if i % 100 == 99:\n",
    "            print(f'[{i + 1}] loss: {running_loss / 100:.3f} | acc: {100.*correct/total:.2f}%')\n",
    "            running_loss = 0.0 # Reset running loss\n",
    "    \n",
    "    return 100. * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation function\n",
    "The evaluation process entails 3 steps:\n",
    "- <span style=\"color: orange;\">**Forward pass**</span> \n",
    "- <span style=\"color: orange;\">**Loss Calculation**</span> \n",
    "\n",
    "There is no need to <span style=\"color: orange;\">**Backward pass**</span> or <span style=\"color: orange;\">**Optimize**</span> our neural network since we are only testing its final state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, testloader, criterion, device):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    test_loss = 0 # Total loss\n",
    "    correct = 0 # Number of correctly predicted images\n",
    "    total = 0 # Total number of images\n",
    "    \n",
    "    # Compute per-class accuracy\n",
    "    class_correct = [0]*10\n",
    "    class_total = [0]*10\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation\n",
    "        # Iterate over the test dataset\n",
    "        for data in testloader: \n",
    "            images, labels = data[0].to(device), data[1].to(device) # data[0] is the image, data[1] is the label\n",
    "            outputs = model(images) # Forward pass\n",
    "            loss = criterion(outputs, labels) # Calculate loss\n",
    "            \n",
    "            # Track test statistics\n",
    "            test_loss += loss.item() # Add loss to test loss\n",
    "            _, predicted = outputs.max(1) # Get the class index with the highest probability\n",
    "            total += labels.size(0) # Add batch size to total\n",
    "            correct += predicted.eq(labels).sum().item() # Add number of correct predictions to correct\n",
    "\n",
    "            # Update per-class statistics\n",
    "            for i in range(len(labels)):  # Loop through each image in the batch\n",
    "                label = labels[i]\n",
    "                class_correct[label] += (predicted[i] == label).item()  # Increment correct count for this class\n",
    "                class_total[label] += 1  # Increment total count for this class\n",
    "\n",
    "    # Calculate overall test accuracy and average loss            \n",
    "    accuracy = 100. * correct / total # Calculate accuracy\n",
    "    avg_loss = test_loss / len(testloader) # Calculate average loss\n",
    "    print(f'Test set: Average loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    print('\\nPer-class accuracy:')\n",
    "    for i in range(10):\n",
    "        if class_total[i] > 0: # Avoid division by zero\n",
    "            class_accuracy = 100. * class_correct[i] / class_total[i]\n",
    "            print(f'{classes[i]:>5s}: {class_accuracy:.2f}%')\n",
    "\n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "This is our main loop. Here we utilize the aforementioned functions to:\n",
    "- <span style=\"color: orange;\">**Train**</span>\n",
    "- <span style=\"color: orange;\">**Evaluate**</span>\n",
    "- <span style=\"color: orange;\">**Benchmark**</span>\n",
    "- <span style=\"color: orange;\">**Save**</span>\n",
    "\n",
    "our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "\n",
      "Epoch 1/5\n",
      "[100] loss: 2.160 | acc: 22.41%\n",
      "[200] loss: 1.798 | acc: 27.06%\n",
      "[300] loss: 1.640 | acc: 30.77%\n",
      "Test set: Average loss: 1.3982, Accuracy: 46.86%\n",
      "Saving best model with accuracy: 46.86%\n",
      "\n",
      "Epoch 2/5\n",
      "[100] loss: 1.469 | acc: 45.88%\n",
      "[200] loss: 1.371 | acc: 47.72%\n",
      "[300] loss: 1.290 | acc: 49.68%\n",
      "Test set: Average loss: 1.0824, Accuracy: 61.09%\n",
      "Saving best model with accuracy: 61.09%\n",
      "\n",
      "Epoch 3/5\n",
      "[100] loss: 1.181 | acc: 58.17%\n",
      "[200] loss: 1.148 | acc: 58.70%\n",
      "[300] loss: 1.110 | acc: 59.53%\n",
      "Test set: Average loss: 1.0197, Accuracy: 64.15%\n",
      "Saving best model with accuracy: 64.15%\n",
      "\n",
      "Epoch 4/5\n",
      "[100] loss: 1.041 | acc: 63.37%\n",
      "[200] loss: 1.041 | acc: 63.47%\n",
      "[300] loss: 1.006 | acc: 63.97%\n",
      "Test set: Average loss: 0.8148, Accuracy: 70.70%\n",
      "Saving best model with accuracy: 70.70%\n",
      "\n",
      "Epoch 5/5\n",
      "[100] loss: 0.962 | acc: 66.15%\n",
      "[200] loss: 0.932 | acc: 66.97%\n",
      "[300] loss: 0.929 | acc: 67.15%\n",
      "Test set: Average loss: 0.8693, Accuracy: 69.72%\n",
      "\n",
      "Training completed in 30.94 minutes\n",
      "Best accuracy: 70.70%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 5 # Number of epochs\n",
    "best_acc = 0 # Best accuracy\n",
    "train_acc_history = [] # Training accuracy history\n",
    "test_acc_history = [] # Test accuracy history\n",
    "\n",
    "print(f\"Training on {device}\") # Print device\n",
    "start_time = time.time() # Start time\n",
    "\n",
    "# Iterate over the epochs\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch {epoch+1}/{num_epochs}') # Print epoch\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    train_acc = train(model, trainloader, criterion, optimizer, device)\n",
    "    test_acc, test_loss = evaluate(model, testloader, criterion, device)\n",
    "    \n",
    "    # Track accuracy history\n",
    "    train_acc_history.append(train_acc)\n",
    "    test_acc_history.append(test_acc)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(test_loss) # Adjust learning rate based on loss\n",
    "    \n",
    "    # Save best model\n",
    "    if test_acc > best_acc:\n",
    "        print(f'Saving best model with accuracy: {test_acc:.2f}%') # Print current best accuracy\n",
    "        # Save the model with the best accuracy in the filename\n",
    "        torch.save(model.state_dict(), f'./cifar10_acc_{test_acc:.2f}.pth')\n",
    "        best_acc = test_acc\n",
    "\n",
    "# Calculate training time\n",
    "training_time = time.time() - start_time # Calculate training time\n",
    "print(f'\\nTraining completed in {training_time/60:.2f} minutes') # Print training time\n",
    "print(f'Best accuracy: {best_acc:.2f}%') # Print final best accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy History\n",
    "Graph the model's accuracy as the epochs progress "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_acc_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mtrain_acc_history\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(test_acc_history, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel Accuracy over Time\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_acc_history' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_acc_history, label='Train Accuracy')\n",
    "plt.plot(test_acc_history, label='Test Accuracy')\n",
    "plt.title('Model Accuracy over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
